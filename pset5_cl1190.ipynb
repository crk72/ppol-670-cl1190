{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Question 1.1]\n",
    "### The value of *k* in KNN Classification\n",
    "When analysing data using nearest-neighbors classification, the optimal value for *k* is going to largely depend on the size of the dataset. Generally speaking, large values of *k* will reduce variability in that it reduces the impact of noise in data, but it will also make classifications less precise.   \n",
    "\n",
    "The value of *k* typically refers to the number of nearest neighbor values to the data point being classified. When implementing KNN classification code, the argument `n_neighbors` is what is used to assign the default value of *k*. The relative weight in contributing to the final classification that each data point has relative to the data point being classified can be assigned using the argument `weight`. The most common values of `weight` are `uniform` (all data points in each neighborhood are weighted equally) and `distance` (the weight of a given data point in a neighborhood is inversely correlated with its distance from the point being classified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prework -- Staging the Environment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load iris dataset\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Convert `species` variable from string to numeric\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "species_encoded=le.fit_transform(iris.species)\n",
    "print(species_encoded)\n",
    "\n",
    "pd.to_numeric(species_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4) \n",
      "y shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "#[Question 1.2] Create feature matrix and target array with `species` as the target\n",
    "\n",
    "X_iris = iris.drop('species', axis=1)\n",
    "Y_iris = species_encoded\n",
    "print('X shape:', X_iris.shape, '\\ny shape:', Y_iris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Question 1.2] Creating a KNN model object where k = 1\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Question 1.3] Fitting the model\n",
    "\n",
    "neigh.fit(X_iris, Y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "#[Question 1.3] Predicting values based on all data available\n",
    "\n",
    "Xpredict = neigh.predict(X_iris)\n",
    "print(Xpredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Question 1.3] Determing accuracy score of the model for k=1\n",
    "\n",
    "accuracy_score(Y_iris, Xpredict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Question 1.3] Determining accuracy score of the model for k=5\n",
    "\n",
    "#Creating new KNN model for k = 5\n",
    "neigh5 = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh5.fit(X_iris, Y_iris)\n",
    "\n",
    "#Predicting values based on new KNN model\n",
    "X5predict = neigh5.predict(X_iris)\n",
    "\n",
    "#Calculate accuracy score for \n",
    "accuracy_score(Y_iris, X5predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Question 1.3]\n",
    "### KNN Models and Accuracy Scores\n",
    "As discussed above, with a larger value for *k*, the impact of noise within the data is decreased, but the boundaries of the classification \"buckets\" are less distinct. Put another way, the larger *k* is, the less precise the boundaries around the classifications are going to be and the more variance there will be within classifications. Thus, the model is less accurate overall. However, this can be partially attributed to the fact that both models are being trained with the full dataset available. Given that this dataset is fairly well-segregated with respect to the `species` variable, it makes sense that a model where *k = 1* is going to be 100% accurate, whereas a model where *k = 5* will have some variance in observations near the boundaries of the `species` categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (75, 4) \n",
      "y train: (75,)\n",
      "X test: (75, 4) \n",
      "y test: (75,)\n"
     ]
    }
   ],
   "source": [
    "#[Question 2.1] Creation of training and test sets from the 'Iris' dataset, with a 50% split and a seed of 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, Y_iris, test_size=0.5, random_state=0)\n",
    "print('X train:', X_train.shape, '\\ny train:', y_train.shape)\n",
    "print('X test:', X_test.shape, '\\ny test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Question 2.2] Creation of KNN model for training data\n",
    "neight5 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#Fitting the model\n",
    "neight5.fit(X_train, y_train)\n",
    "\n",
    "#Predicting test data\n",
    "\n",
    "NE5predict = neight5.predict(X_test)\n",
    "\n",
    "#[Question 2.3] Computing accuracy score\n",
    "\n",
    "accuracy_score(y_test, NE5predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Question 2.3]\n",
    "\n",
    "### Why the accuracy scores from 1.3 and 2.3 are the same\n",
    "The accuracy score obtained in Question 1.3 is essentially equivalent to the accuracy score obtained in Question 2.3. This is because the *k*-value of the model in Q1.3 is the same as the *k*-value of the model in Q2.3, and the training data in Q2.3 effectively taught the algorithm via the training data batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Question 2.4]\n",
    "\n",
    "### The merits of `cross_val_score`\n",
    "\n",
    "Cross-validation (CV) is a process that helps protect against overfitting a given model and \"knowledge leaks\" from the test data to a model during the learning process, while allowing for overall effective machine learning.\n",
    "\n",
    "In order to best protect against overfitting, data scientists can split their data into three batches: the training batch (from which the algorithm learns about the data), a validation batch (to act as a prophylactic measure against the algorithm accidentally learning about test data during the model hyperparameter turning process), and the test batch (the data on which the algorithm will actually run). This process reduces the amount of data from which the algorithm can learn and can also potentially end with misleading results due to random chance.\n",
    "\n",
    "CV sidesteps these issues by splitting the training batch into sub-batches. In *k*-fold CV, the training set is split up into *k* number of sub-batches. The algorithm is then trained on *k - 1* of those sub-batches, and validated on the last sub-batch.\n",
    "\n",
    "CV is used in situations in which it would be methodologically unsound to split a dataset into three discrete batches (e.g., small datasets). It could also be used in scenarios in which the analyst is trying to find the most effective value of *k* for a large dataset that is not well self-segregated with respect to the variable of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.93333333, 0.96666667, 1.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Question 2.5] Creating a KNN model with k = 3\n",
    "\n",
    "KNN3 = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Running cross-validation on the model\n",
    "\n",
    "cross_val_score(KNN3, X_iris, Y_iris, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Performance of the KNN3 model\n",
    "\n",
    "Based on the CV scores returned by `cross_val_score`, my model performs fairly well, accurately classifying query points over 93% each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9        0.9        0.83333333 0.93333333 0.9       ]\n",
      "[0.93333333 1.         0.93333333 0.96666667 1.        ]\n",
      "[0.96666667 0.96666667 0.93333333 0.96666667 1.        ]\n",
      "[0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "[0.96666667 0.93333333 0.93333333 0.9        1.        ]\n",
      "[0.96666667 0.96666667 0.93333333 0.93333333 1.        ]\n"
     ]
    }
   ],
   "source": [
    "#Bonus Round: Scripting a function\n",
    "#Variables involved: \n",
    "#    list_of_k: List of k-values to be tested\n",
    "#    cvfold: Number of sub-batches desired for CV testing\n",
    "#    xparam: Independent variable\n",
    "#    yparam: Variable of interest/classification\n",
    "\n",
    "def knncv(list_of_k, cvfold, xparam, yparam):\n",
    "    for k in list_of_k:\n",
    "        KNN = KNeighborsClassifier(n_neighbors=k)\n",
    "        print(cross_val_score(KNN, xparam, yparam, cv=cvfold))\n",
    "\n",
    "\n",
    "#Demonstration of use -- quick testing of multiple k-scores to determine most accurate value for k of those scores.\n",
    "knncv([73, 15, 3, 5, 2, 1], 5, X_iris, Y_iris)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
